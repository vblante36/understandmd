{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UnderstandMD: Translating Clinical Language into Plain English with Generative AI\n",
    "\n",
    "This project explores the use of a generative AI model to convert clinical medical descriptions into plain-language explanations understandable by a 12-year-old. The goal is to support health literacy, reduce confusion, and make health information more accessible to the general public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model path: /Users/victoriablante/1-MSDS/summer25/understandmd/models/mistral-7b-instruct-v0.1.Q3_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "# === Core Libraries ===\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "model_path = os.getenv(\"LLAMA_MODEL_PATH\")\n",
    "print(\"Loaded model path:\", model_path)\n",
    "\n",
    "# === Data & Utility Libraries ===\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Audio Processing ===\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# === Text-to-Speech (Offline) ===\n",
    "import pyttsx3\n",
    "from scipy.io.wavfile import read\n",
    "\n",
    "# === Gradio Interface ===\n",
    "import gradio as gr\n",
    "\n",
    "# === Fuzzy Matching ===\n",
    "from rapidfuzz import process\n",
    "\n",
    "# === Local LLM: Mistral 7B with llama-cpp ===\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Validate model path\n",
    "if not model_path or not os.path.isfile(model_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Model file not found.\\n\\n\"\n",
    "        f\"Expected at: {model_path}\\n\"\n",
    "        f\"Tip: Check your `.env` file in `understandmd/.env` and verify that LLAMA_MODEL_PATH is set correctly.\"\n",
    "    )\n",
    "\n",
    "# Load the Llama model\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,\n",
    "    n_threads=8,\n",
    "    n_batch=512,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Introduction\n",
    "\n",
    "Medical information is often written in complex, technical language that can be difficult for patients to understand. This communication gap can lead to confusion, reduced health literacy, and poorer health outcomes, especially for those without medical backgrounds.\n",
    "\n",
    "This project aims to bridge that gap by using a locally hosted generative AI model to translate clinical descriptions into simple, easy-to-understand explanations suitable for a 12-year-old. It also highlights common symptoms and converts the explanation into spoken audio using text-to-speech, making the content more accessible to users with reading difficulties, visual impairments, or different learning preferences.\n",
    "\n",
    "By leveraging the Mistral-7B-Instruct model, we ensure that all processing happens privately and cost-free without relying on cloud-based APIs. The result is a reproducible, user-friendly system that supports health literacy and empowers patients to better understand their own care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "This project uses two complementary datasets to support the generation and evaluation of plain-language medical explanations:\n",
    "\n",
    "### 1. **Manually Curated Dataset (50 entries)**  \n",
    "This dataset was created using public clinical descriptions from the **Mayo Clinic** website. Each row includes:\n",
    "- `condition`: The name of the medical condition  \n",
    "- `clinical_description`: A technical or clinical explanation of the condition  \n",
    "- `simple_explanation`: A human-written, gold-standard explanation in plain language (optional)\n",
    "\n",
    "This dataset serves as a benchmark for prompt testing and model evaluation.  \n",
    "*Limitations:* Only a subset of conditions includes gold-standard outputs, and the complexity of descriptions varies.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Scraped Dataset (MedlinePlus)**  \n",
    "To expand coverage, medical condition pages were scraped from [MedlinePlus](https://medlineplus.gov/), a trusted health resource maintained by the U.S. National Library of Medicine. This dataset includes:\n",
    "- `term`: The condition name  \n",
    "- `description`: A brief clinical summary extracted from each page\n",
    "\n",
    "This larger dataset supports real-time term matching, including for slightly misspelled inputs.  \n",
    "*Limitations:* Descriptions may be brief and vary in specificity or clarity.\n",
    "\n",
    "---\n",
    "\n",
    "Together, these datasets enable both robust prompt engineering and wide user-facing coverage, helping ensure that even unfamiliar or imperfect terms can return accurate, readable explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_df = pd.read_csv('clinical_conditions.csv')\n",
    "clinical_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_df = pd.read_csv(\"medline_conditions.csv\")\n",
    "known_terms = medical_df['term'].dropna().str.lower().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Before generating explanations, we performed light preprocessing to clean and standardize the dataset:\n",
    "\n",
    "- Removed newline characters from clinical descriptions\n",
    "- Collapsed extra spaces into single spaces\n",
    "- Stripped leading and trailing whitespace from all text fields\n",
    "\n",
    "This helped ensure that the input text fed into the language model was consistent and clean, minimizing formatting artifacts that could affect generation quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove newlines, strip leading/trailing spaces, and collapse double spaces\n",
    "    return ' '.join(text.replace('\\n', ' ').split())\n",
    "\n",
    "clinical_df['clinical_description'] = clinical_df['clinical_description'].apply(clean_text)\n",
    "\n",
    "clinical_df['condition'] = clinical_df['condition'].str.strip()\n",
    "clinical_df['clinical_description'] = clinical_df['clinical_description'].str.strip()\n",
    "\n",
    "print(f\"Dataset ready. Total rows: {len(clinical_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "We used the Mistral-7B-Instruct model in GGUF format, hosted locally with the `llama-cpp-python` library. This instruction-tuned model is well-suited for tasks like \"Explain in simple terms,\" making it ideal for translating clinical language into plain English.\n",
    "\n",
    "The model was run fully offline on an Apple M3 Pro chip using 8 threads and a quantized 4-bit version to optimize for CPU performance. This setup allows us to generate responses efficiently without relying on API keys, cloud services, or paid infrastructure.\n",
    "\n",
    "Running the model locally also ensures user privacy and makes the system fully reproducible for anyone with the required hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"You are a health explainer. Translate the following medical term into plain English so a 12-year-old can understand it.\n",
    "\n",
    "Term: Atrial fibrillation\n",
    "Description: Atrial fibrillation is an irregular and often very rapid heart rhythm. It can lead to blood clots, stroke, heart failure, and other complications.\n",
    "\n",
    "Plain Explanation:\"\"\"\n",
    "\n",
    "output = llm(test_prompt, max_tokens=150, stop=[\"\\n\\n\"])\n",
    "print(output[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering and Methodology\n",
    "\n",
    "I designed structured prompts that ask the model to output both a plain-language explanation and a list of symptoms. The system handles cases where a description is provided, as well as when only a term is available.\n",
    "\n",
    "The format uses role-based chat messages to guide the model and ensures consistent output formatting for downstream processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fuzzy match input ===\n",
    "def match_term(user_input, threshold=90):\n",
    "    result = process.extractOne(user_input, known_terms)\n",
    "    if result:\n",
    "        match, score, _ = result\n",
    "        if score >= threshold:\n",
    "            return match\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_with_mistral(prompt, max_tokens=280):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a health explainer who explains medical conditions to 12-year-olds.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    try:\n",
    "        result = llm.create_chat_completion(messages=messages, max_tokens=max_tokens)\n",
    "        response_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return {\"choices\": [{\"text\": response_text}]}\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] LLM generation failed: {e}\")\n",
    "        return {\"choices\": [{\"text\": \"\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_explanation_then_symptoms(term, desc, llm, max_tokens=280):\n",
    "    if desc and desc.strip():  # Use the description if available\n",
    "        prompt = f\"\"\"You are a health explainer. Given the medical term and its description, do two things:\n",
    "\n",
    "1. Explain the condition so a 12-year-old can understand it.\n",
    "2. Then list **up to 4 common symptoms** in simple language using bullet points.\n",
    "\n",
    "Format your response like this:\n",
    "\n",
    "Plain Explanation: ...\n",
    "Symptoms:\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "Term: {term}\n",
    "Description: {desc}\n",
    "\n",
    "Your response:\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"You are a health explainer. Given a medical term, do two things:\n",
    "\n",
    "1. Explain the condition so a 12-year-old can understand it.\n",
    "2. Then list **up to 4 common symptoms** in simple language using bullet points.\n",
    "\n",
    "Format your response like this:\n",
    "\n",
    "Plain Explanation: ...\n",
    "Symptoms:\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "Term: {term}\n",
    "\n",
    "Your response:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm(prompt, max_tokens=max_tokens)\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_explanation_and_symptoms(text):\n",
    "    if \"Symptoms:\" in text:\n",
    "        explanation, symptoms = text.split(\"Symptoms:\", 1)\n",
    "        explanation = explanation.replace(\"Plain Explanation:\", \"\").strip()\n",
    "        symptoms = symptoms.strip()\n",
    "    else:\n",
    "        explanation = text.strip()\n",
    "        symptoms = \"\"\n",
    "    return explanation, symptoms\n",
    "\n",
    "explanations = []\n",
    "symptom_lists = []\n",
    "\n",
    "for i, row in tqdm(clinical_df.iterrows(), total=len(clinical_df)):\n",
    "    term = row[\"condition\"]  # or \"term\"\n",
    "    desc = row[\"clinical_description\"]\n",
    "    full_text = generate_explanation_then_symptoms(term, desc, llm, max_tokens=280)\n",
    "    explanation, symptoms = split_explanation_and_symptoms(full_text)\n",
    "    explanations.append(explanation)\n",
    "    symptom_lists.append(symptoms)\n",
    "\n",
    "clinical_df[\"plain_explanation\"] = explanations\n",
    "clinical_df[\"symptoms\"] = symptom_lists\n",
    "clinical_df.to_csv(\"understandmd_generated.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and Results\n",
    "\n",
    "I tested the model on a set of 50 medical conditions. For each one, the system generated a plain-language explanation and a list of symptoms. The responses were consistently understandable and aligned well with a 12-year-old reading level.\n",
    "\n",
    "Below are selected examples that illustrate how the model translates clinical language into accessible explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv('understandmd_generated.csv')\n",
    "# Ensure full text is shown in notebook output\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few example rows\n",
    "example_rows = new_df[['condition', 'clinical_description', 'plain_explanation', 'symptoms']].iloc[[0, 5, 22, 37]]\n",
    "display(example_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-Speech Generation\n",
    "\n",
    "Medical explanations are not always accessible to individuals with visual impairments, low literacy, or different learning preferences. To make the output more inclusive, we convert plain-language explanations into natural-sounding audio using the `pyttsx3` text-to-speech library, which works fully offline.\n",
    "\n",
    "We format the model’s output into a complete sentence that combines the explanation and a list of symptoms in a natural way. The result is saved as a `.wav` file that can be played directly, making the information easier to access for a wide range of users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_speech_text(explanation, symptoms):\n",
    "    # Clean up symptom bullets and join them naturally\n",
    "    if symptoms:\n",
    "        symptoms_list = [line.strip(\"- \").strip() for line in symptoms.split(\"\\n\") if line.strip()]\n",
    "        if len(symptoms_list) == 1:\n",
    "            symptom_str = symptoms_list[0]\n",
    "        else:\n",
    "            symptom_str = \", \".join(symptoms_list[:-1]) + \", and \" + symptoms_list[-1]\n",
    "        speech = f\"{explanation} Symptoms include: {symptom_str}.\"\n",
    "    else:\n",
    "        speech = explanation\n",
    "    return speech\n",
    "\n",
    "\n",
    "def speak_with_pyttsx3(text, rate=150):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty(\"rate\", rate)\n",
    "\n",
    "    # Save as AIFF\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".aiff\") as aiff_file:\n",
    "        aiff_path = aiff_file.name\n",
    "\n",
    "    engine.save_to_file(text, aiff_path)\n",
    "    engine.runAndWait()\n",
    "\n",
    "    # Wait a moment to ensure file is written (esp. on macOS)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if not os.path.exists(aiff_path) or os.path.getsize(aiff_path) < 1000:\n",
    "        raise RuntimeError(\"AIFF file not written or empty\")\n",
    "\n",
    "    # Convert to proper WAV format\n",
    "    sound = AudioSegment.from_file(aiff_path, format=\"aiff\")\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as wav_file:\n",
    "        wav_path = wav_file.name\n",
    "    sound.export(wav_path, format=\"wav\")\n",
    "\n",
    "    # Load WAV\n",
    "    sr, audio_np = read(wav_path)\n",
    "    if audio_np.size == 0:\n",
    "        raise RuntimeError(\"WAV file exists but is silent\")\n",
    "    return (sr, audio_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Inference Pipeline\n",
    "\n",
    "The pipeline performs three key tasks:\n",
    "\n",
    "1. **Fuzzy match** the user input to known medical terms.\n",
    "2. **Generate** a plain-language explanation and list of symptoms using the LLM.\n",
    "3. **Convert** the explanation to audio using text-to-speech.\n",
    "\n",
    "This pipeline ensures users receive accessible, understandable, and spoken explanations of medical terms with no need for internet access or external APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(user_input, desc):\n",
    "    term = match_term(user_input) or user_input\n",
    "    raw_response = generate_explanation_then_symptoms(term, desc, run_llm_with_mistral)\n",
    "    explanation, symptoms = split_explanation_and_symptoms(raw_response)\n",
    "    # NEW: Combine explanation + symptoms into one spoken paragraph\n",
    "    speech_text = prepare_speech_text(explanation, symptoms)\n",
    "    try:\n",
    "        audio = speak_with_pyttsx3(speech_text, rate=150)\n",
    "    except Exception as e:\n",
    "        audio = None\n",
    "\n",
    "    return explanation, symptoms, audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Interface\n",
    "\n",
    "To make the system user-friendly and accessible to a non-technical audience, we built an interactive interface using Gradio.\n",
    "\n",
    "The interface includes:\n",
    "\n",
    "- A **text input** for the user to enter a medical term.\n",
    "- An optional field to provide additional **clinical context**.\n",
    "- A **\"Generate Explanation\"** button styled in soft blue for clarity and visibility.\n",
    "- Output sections that display:\n",
    "  - A plain-language **explanation**,\n",
    "  - A list of **common symptoms**,\n",
    "  - And a **playable audio file** of the explanation.\n",
    "\n",
    "The interface uses custom CSS for a clean, branded look and responds to both button clicks and Enter key submissions.\n",
    "\n",
    "This no-code UI allows anyone to interact with the generative model without needing to install libraries, write code, or understand AI—making it an ideal tool for improving health communication at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blue colors\n",
    "soft_blue = \"#6366f1\"\n",
    "light_blue_bg = \"rgba(59, 130, 246, 0.1)\"\n",
    "\n",
    "# Custom CSS to override the Soft theme's blue label pills\n",
    "custom_css = f\"\"\"\n",
    "/* Title color */\n",
    "h1 {{\n",
    "    color: {soft_blue} !important;\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "    text-align: center;\n",
    "    margin-bottom: 24px;\n",
    "}}\n",
    "\n",
    "/* Button styling */\n",
    "#generate-button {{\n",
    "    background-color: {light_blue_bg} !important;\n",
    "    color: {soft_blue} !important;\n",
    "    font-weight: bold !important;\n",
    "    font-size: 16px !important;\n",
    "    border: 2px solid {soft_blue} !important;\n",
    "    border-radius: 14px !important;\n",
    "    padding: 14px 24px !important;\n",
    "    width: 100%;\n",
    "    text-align: center;\n",
    "    box-shadow: 0px 2px 6px rgba(0,0,0,0.1);\n",
    "    transition: background-color 0.3s ease;\n",
    "}}\n",
    "\n",
    "#generate-button:hover {{\n",
    "    background-color: rgba(79, 70, 229, 0.15) !important;\n",
    "}}\n",
    "\"\"\"\n",
    "# Gradio app\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=custom_css) as demo:\n",
    "    gr.Markdown(\"# UnderstandMD: Medical Terms Made Simple\")\n",
    "\n",
    "    with gr.Row():\n",
    "        input_box = gr.Textbox(label=\"Medical Term\", placeholder=\"e.g., asthma\", scale=2)\n",
    "        desc_box = gr.Textbox(label=\"(Optional) Add a Description\", placeholder=\"Add any specific context\")\n",
    "\n",
    "    submit_btn = gr.Button(\"Generate Explanation\", elem_id=\"generate-button\")\n",
    "\n",
    "    with gr.Column():\n",
    "        output_text = gr.Textbox(label=\"Plain Explanation\")\n",
    "        output_symptoms = gr.Textbox(label=\"Symptoms\")\n",
    "        output_audio = gr.Audio(label=\"Listen\", type=\"filepath\")\n",
    "\n",
    "    submit_btn.click(fn=full_pipeline, inputs=[input_box, desc_box], outputs=[output_text, output_symptoms, output_audio])\n",
    "    input_box.submit(fn=full_pipeline, inputs=[input_box, desc_box], outputs=[output_text, output_symptoms, output_audio])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project shows that a locally hosted generative AI model can successfully simplify complex medical language. Using Mistral-7B-Instruct and carefully designed prompts, we translated clinical descriptions into plain-language explanations with minimal errors.\n",
    "\n",
    "To further enhance accessibility, I added a text-to-speech component using `pyttsx3`, allowing users to listen to the explanations. This supports individuals who prefer auditory information or may struggle with reading medical content.\n",
    "\n",
    "The resulting system is cost-free, privacy-preserving, and easily extendable. Future improvements could include multilingual support, full offline functionality, integration into patient-facing tools, or evaluation with real user feedback.\n",
    "\n",
    "Overall, this work highlights the potential of generative AI to make healthcare communication more understandable and inclusive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
